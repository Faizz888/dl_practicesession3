{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6287e4",
   "metadata": {},
   "source": [
    "# Part 2.3: RNN vs LSTM vs GRU Comparison\n",
    "\n",
    "Comprehensive comparison of RNN architectures with hands-on implementation and analysis.\n",
    "\n",
    "## Objective\n",
    "- Implement vanilla RNN, LSTM, and GRU from scratch and using PyTorch\n",
    "- Compare on sequence modeling tasks\n",
    "- Analyze gate activations and learning dynamics\n",
    "- Perform ablation study on gate mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"RNN Architecture Comparison\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom LSTM implementation\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Forget gate\n",
    "        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # Input gate\n",
    "        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # Candidate values\n",
    "        self.W_C = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # Output gate\n",
    "        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # Store gate activations for visualization\n",
    "        self.gate_activations = {\n",
    "            'forget': [],\n",
    "            'input': [],\n",
    "            'output': [],\n",
    "            'cell': []\n",
    "        }\n",
    "        \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "            C_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h_t, C_t = hidden_state\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            combined = torch.cat([x_t, h_t], dim=1)\n",
    "            \n",
    "            # LSTM gates\n",
    "            f_t = torch.sigmoid(self.W_f(combined))  # Forget gate\n",
    "            i_t = torch.sigmoid(self.W_i(combined))  # Input gate\n",
    "            C_tilde = torch.tanh(self.W_C(combined))  # Candidate values\n",
    "            o_t = torch.sigmoid(self.W_o(combined))  # Output gate\n",
    "            \n",
    "            # Update cell state\n",
    "            C_t = f_t * C_t + i_t * C_tilde\n",
    "            \n",
    "            # Update hidden state\n",
    "            h_t = o_t * torch.tanh(C_t)\n",
    "            \n",
    "            outputs.append(h_t)\n",
    "            \n",
    "            # Store activations for analysis (only for last batch)\n",
    "            if t == seq_len - 1:\n",
    "                self.gate_activations['forget'].append(f_t.mean().item())\n",
    "                self.gate_activations['input'].append(i_t.mean().item())\n",
    "                self.gate_activations['output'].append(o_t.mean().item())\n",
    "                self.gate_activations['cell'].append(C_t.mean().item())\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, (h_t, C_t)\n",
    "\n",
    "# Custom GRU implementation\n",
    "class CustomGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Reset gate\n",
    "        self.W_r = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # Update gate\n",
    "        self.W_z = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # New gate\n",
    "        self.W_n = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # Store gate activations\n",
    "        self.gate_activations = {\n",
    "            'reset': [],\n",
    "            'update': []\n",
    "        }\n",
    "        \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h_t = hidden_state\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            combined = torch.cat([x_t, h_t], dim=1)\n",
    "            \n",
    "            # GRU gates\n",
    "            r_t = torch.sigmoid(self.W_r(combined))  # Reset gate\n",
    "            z_t = torch.sigmoid(self.W_z(combined))  # Update gate\n",
    "            \n",
    "            # New gate with reset applied\n",
    "            combined_new = torch.cat([x_t, r_t * h_t], dim=1)\n",
    "            n_t = torch.tanh(self.W_n(combined_new))\n",
    "            \n",
    "            # Update hidden state\n",
    "            h_t = (1 - z_t) * n_t + z_t * h_t\n",
    "            \n",
    "            outputs.append(h_t)\n",
    "            \n",
    "            # Store activations\n",
    "            if t == seq_len - 1:\n",
    "                self.gate_activations['reset'].append(r_t.mean().item())\n",
    "                self.gate_activations['update'].append(z_t.mean().item())\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, h_t\n",
    "\n",
    "# Vanilla RNN for comparison\n",
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_hh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_xh = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h_t = hidden_state\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            h_t = torch.tanh(self.W_xh(x_t) + self.W_hh(h_t))\n",
    "            outputs.append(h_t)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, h_t\n",
    "\n",
    "print(\"Custom RNN, LSTM, and GRU implementations ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3cda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Classification Dataset\n",
    "class SequenceClassificationDataset(Dataset):\n",
    "    def __init__(self, num_samples=2000, seq_length=50, num_features=20, num_classes=3):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Generate synthetic sequential data\n",
    "        self.data, self.labels = self._generate_data()\n",
    "        \n",
    "    def _generate_data(self):\n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(self.num_samples):\n",
    "            # Generate class\n",
    "            class_id = np.random.randint(0, self.num_classes)\n",
    "            \n",
    "            # Generate sequence with class-dependent patterns\n",
    "            if class_id == 0:\n",
    "                # Increasing trend\n",
    "                base_seq = np.linspace(0, 1, self.seq_length)\n",
    "                noise = np.random.normal(0, 0.1, (self.seq_length, self.num_features))\n",
    "                seq = base_seq.reshape(-1, 1) + noise\n",
    "            elif class_id == 1:\n",
    "                # Sinusoidal pattern\n",
    "                t = np.linspace(0, 4*np.pi, self.seq_length)\n",
    "                base_seq = np.sin(t)\n",
    "                noise = np.random.normal(0, 0.1, (self.seq_length, self.num_features))\n",
    "                seq = base_seq.reshape(-1, 1) + noise\n",
    "            else:\n",
    "                # Random walk\n",
    "                steps = np.random.normal(0, 0.1, self.seq_length)\n",
    "                base_seq = np.cumsum(steps)\n",
    "                noise = np.random.normal(0, 0.05, (self.seq_length, self.num_features))\n",
    "                seq = base_seq.reshape(-1, 1) + noise\n",
    "            \n",
    "            data.append(torch.FloatTensor(seq))\n",
    "            labels.append(torch.LongTensor([class_id]))\n",
    "            \n",
    "        return data, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Model wrapper for comparison\n",
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, rnn_type, input_size, hidden_size, num_classes):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        if rnn_type == 'RNN':\n",
    "            self.rnn = CustomRNN(input_size, hidden_size)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = CustomLSTM(input_size, hidden_size)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = CustomGRU(input_size, hidden_size)\n",
    "        elif rnn_type == 'PyTorch_LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        elif rnn_type == 'PyTorch_GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get RNN output\n",
    "        if self.rnn_type in ['PyTorch_LSTM', 'PyTorch_GRU']:\n",
    "            output, _ = self.rnn(x)\n",
    "        else:\n",
    "            output, _ = self.rnn(x)\n",
    "        \n",
    "        # Use last timestep for classification\n",
    "        last_output = output[:, -1, :]\n",
    "        return self.classifier(last_output)\n",
    "\n",
    "print(\"Dataset and model wrapper ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).squeeze()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(1) == targets).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device).squeeze()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(1) == targets).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'training_time': training_time,\n",
    "        'final_val_acc': val_accs[-1]\n",
    "    }\n",
    "\n",
    "# Experimental setup\n",
    "input_size = 20\n",
    "hidden_size = 64\n",
    "num_classes = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Create datasets\n",
    "dataset = SequenceClassificationDataset(num_samples=2000, seq_length=50, \n",
    "                                      num_features=input_size, num_classes=num_classes)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Dataset created: {len(train_dataset)} training, {len(val_dataset)} validation samples\")\n",
    "print(f\"Sequence length: 50, Features: {input_size}, Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different architectures\n",
    "architectures = ['RNN', 'LSTM', 'GRU', 'PyTorch_LSTM', 'PyTorch_GRU']\n",
    "results = {}\n",
    "\n",
    "print(\"\\nStarting architecture comparison...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\nðŸ”„ Training {arch}...\")\n",
    "    \n",
    "    model = SequenceClassifier(arch, input_size, hidden_size, num_classes).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    result = train_model(model, train_loader, val_loader, num_epochs=50)\n",
    "    result['num_params'] = num_params\n",
    "    result['model'] = model\n",
    "    \n",
    "    results[arch] = result\n",
    "    \n",
    "    print(f\"âœ… {arch} completed - Final Accuracy: {result['final_val_acc']:.3f}, Time: {result['training_time']:.1f}s\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All architectures trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('RNN Architecture Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "arch_colors = dict(zip(architectures, colors))\n",
    "\n",
    "# 1. Validation Accuracy Curves\n",
    "ax = axes[0, 0]\n",
    "for arch in architectures:\n",
    "    val_accs = results[arch]['val_accs']\n",
    "    ax.plot(val_accs, color=arch_colors[arch], linewidth=2, label=arch)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Accuracy')\n",
    "ax.set_title('Learning Progress Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Final Performance vs Parameters\n",
    "ax = axes[0, 1]\n",
    "final_accs = [results[arch]['final_val_acc'] for arch in architectures]\n",
    "num_params = [results[arch]['num_params'] for arch in architectures]\n",
    "\n",
    "scatter = ax.scatter(num_params, final_accs, c=colors, s=150, alpha=0.7)\n",
    "for i, arch in enumerate(architectures):\n",
    "    ax.annotate(arch, (num_params[i], final_accs[i]), \n",
    "               xytext=(5, 5), textcoords='offset points', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Number of Parameters')\n",
    "ax.set_ylabel('Final Validation Accuracy')\n",
    "ax.set_title('Performance vs Complexity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Training Time Comparison\n",
    "ax = axes[0, 2]\n",
    "training_times = [results[arch]['training_time'] for arch in architectures]\n",
    "\n",
    "bars = ax.bar(architectures, training_times, color=colors, alpha=0.7)\n",
    "ax.set_ylabel('Training Time (seconds)')\n",
    "ax.set_title('Training Efficiency')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time_val in zip(bars, training_times):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "           f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Gate Activations (LSTM)\n",
    "ax = axes[1, 0]\n",
    "lstm_model = results['LSTM']['model']\n",
    "if hasattr(lstm_model.rnn, 'gate_activations'):\n",
    "    gates = lstm_model.rnn.gate_activations\n",
    "    epochs = range(len(gates['forget']))\n",
    "    \n",
    "    ax.plot(epochs, gates['forget'], label='Forget Gate', linewidth=2)\n",
    "    ax.plot(epochs, gates['input'], label='Input Gate', linewidth=2)\n",
    "    ax.plot(epochs, gates['output'], label='Output Gate', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Training Progress')\n",
    "    ax.set_ylabel('Average Gate Activation')\n",
    "    ax.set_title('LSTM Gate Activations')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Architecture Summary Table\n",
    "ax = axes[1, 1]\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = []\n",
    "for arch in architectures:\n",
    "    result = results[arch]\n",
    "    table_data.append([\n",
    "        arch,\n",
    "        f\"{result['final_val_acc']:.3f}\",\n",
    "        f\"{result['num_params']:,}\",\n",
    "        f\"{result['training_time']:.1f}s\"\n",
    "    ])\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                colLabels=['Architecture', 'Accuracy', 'Parameters', 'Time'],\n",
    "                cellLoc='center',\n",
    "                loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1.2, 2)\n",
    "ax.set_title('Performance Summary', fontweight='bold')\n",
    "\n",
    "# 6. Convergence Rate Analysis\n",
    "ax = axes[1, 2]\n",
    "# Find epoch where each model reaches 80% of final accuracy\n",
    "convergence_epochs = []\n",
    "for arch in architectures:\n",
    "    val_accs = results[arch]['val_accs']\n",
    "    target_acc = 0.8 * val_accs[-1]\n",
    "    \n",
    "    conv_epoch = len(val_accs)  # Default to end\n",
    "    for i, acc in enumerate(val_accs):\n",
    "        if acc >= target_acc:\n",
    "            conv_epoch = i + 1\n",
    "            break\n",
    "    convergence_epochs.append(conv_epoch)\n",
    "\n",
    "bars = ax.bar(architectures, convergence_epochs, color=colors, alpha=0.7)\n",
    "ax.set_ylabel('Epochs to Convergence')\n",
    "ax.set_title('Convergence Speed')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, epochs in zip(bars, convergence_epochs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "           f'{epochs}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479b20f",
   "metadata": {},
   "source": [
    "## Ablation Study: Gate Mechanisms\n",
    "\n",
    "Let's analyze the importance of different gate mechanisms by selectively disabling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation study - Modified LSTM with disabled gates\n",
    "class AblationLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, disable_forget=False, disable_input=False, disable_output=False):\n",
    "        super(AblationLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.disable_forget = disable_forget\n",
    "        self.disable_input = disable_input\n",
    "        self.disable_output = disable_output\n",
    "        \n",
    "        # Gates\n",
    "        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_C = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "            C_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h_t, C_t = hidden_state\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            combined = torch.cat([x_t, h_t], dim=1)\n",
    "            \n",
    "            # Gates with ablations\n",
    "            f_t = torch.ones_like(C_t) if self.disable_forget else torch.sigmoid(self.W_f(combined))\n",
    "            i_t = torch.ones_like(C_t) if self.disable_input else torch.sigmoid(self.W_i(combined))\n",
    "            C_tilde = torch.tanh(self.W_C(combined))\n",
    "            o_t = torch.ones_like(h_t) if self.disable_output else torch.sigmoid(self.W_o(combined))\n",
    "            \n",
    "            # Update cell state\n",
    "            C_t = f_t * C_t + i_t * C_tilde\n",
    "            \n",
    "            # Update hidden state\n",
    "            h_t = o_t * torch.tanh(C_t)\n",
    "            \n",
    "            outputs.append(h_t)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, (h_t, C_t)\n",
    "\n",
    "# Ablation configurations\n",
    "ablation_configs = {\n",
    "    'Full LSTM': {'disable_forget': False, 'disable_input': False, 'disable_output': False},\n",
    "    'No Forget': {'disable_forget': True, 'disable_input': False, 'disable_output': False},\n",
    "    'No Input': {'disable_forget': False, 'disable_input': True, 'disable_output': False},\n",
    "    'No Output': {'disable_forget': False, 'disable_input': False, 'disable_output': True},\n",
    "    'Forget+Input Only': {'disable_forget': False, 'disable_input': False, 'disable_output': True},\n",
    "}\n",
    "\n",
    "ablation_results = {}\n",
    "\n",
    "print(\"\\nStarting LSTM Ablation Study...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for config_name, config in ablation_configs.items():\n",
    "    print(f\"\\nðŸ”„ Testing: {config_name}\")\n",
    "    \n",
    "    # Create model with ablation\n",
    "    class AblationClassifier(nn.Module):\n",
    "        def __init__(self, **kwargs):\n",
    "            super().__init__()\n",
    "            self.rnn = AblationLSTM(input_size, hidden_size, **kwargs)\n",
    "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            output, _ = self.rnn(x)\n",
    "            last_output = output[:, -1, :]\n",
    "            return self.classifier(last_output)\n",
    "    \n",
    "    model = AblationClassifier(**config).to(device)\n",
    "    result = train_model(model, train_loader, val_loader, num_epochs=30)\n",
    "    \n",
    "    ablation_results[config_name] = result\n",
    "    print(f\"âœ… {config_name}: Final Accuracy = {result['final_val_acc']:.3f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Ablation study completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0716c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ablation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('LSTM Ablation Study Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "configs = list(ablation_results.keys())\n",
    "ablation_colors = plt.cm.Set3(np.linspace(0, 1, len(configs)))\n",
    "\n",
    "# 1. Validation accuracy curves\n",
    "ax = axes[0]\n",
    "for i, config in enumerate(configs):\n",
    "    val_accs = ablation_results[config]['val_accs']\n",
    "    ax.plot(val_accs, color=ablation_colors[i], linewidth=2, label=config)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Accuracy')\n",
    "ax.set_title('Learning Curves - Gate Ablation')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Final performance comparison\n",
    "ax = axes[1]\n",
    "final_accs = [ablation_results[config]['final_val_acc'] for config in configs]\n",
    "\n",
    "bars = ax.bar(configs, final_accs, color=ablation_colors, alpha=0.7)\n",
    "ax.set_ylabel('Final Validation Accuracy')\n",
    "ax.set_title('Gate Importance Analysis')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, final_accs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "           f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nðŸ“Š ABLATION STUDY SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for config, result in ablation_results.items():\n",
    "    print(f\"{config:20s}: {result['final_val_acc']:.3f} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b564220",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis: RNN vs LSTM vs GRU\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "#### 1. Performance Comparison\n",
    "- **LSTM**: Best overall performance due to sophisticated gating mechanism\n",
    "- **GRU**: Close performance to LSTM with fewer parameters (more efficient)\n",
    "- **Vanilla RNN**: Struggles with longer sequences due to vanishing gradients\n",
    "\n",
    "#### 2. Architectural Differences\n",
    "\n",
    "**LSTM (Long Short-Term Memory)**:\n",
    "- **3 gates**: Forget, Input, Output\n",
    "- **Cell state**: Separate memory pathway\n",
    "- **Equations**:\n",
    "  - $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$ (Forget gate)\n",
    "  - $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$ (Input gate)\n",
    "  - $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$ (Candidate values)\n",
    "  - $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$ (Cell state)\n",
    "  - $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$ (Output gate)\n",
    "  - $h_t = o_t * \\tanh(C_t)$ (Hidden state)\n",
    "\n",
    "**GRU (Gated Recurrent Unit)**:\n",
    "- **2 gates**: Reset, Update\n",
    "- **No separate cell state**\n",
    "- **Equations**:\n",
    "  - $r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$ (Reset gate)\n",
    "  - $z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$ (Update gate)\n",
    "  - $\\tilde{h}_t = \\tanh(W \\cdot [r_t * h_{t-1}, x_t])$ (New gate)\n",
    "  - $h_t = (1-z_t) * \\tilde{h}_t + z_t * h_{t-1}$ (Hidden state)\n",
    "\n",
    "**Vanilla RNN**:\n",
    "- **Simple recurrence**: $h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$\n",
    "- **No gating mechanism**\n",
    "\n",
    "#### 3. Ablation Study Insights\n",
    "- **Forget gate**: Most critical for long-term dependencies\n",
    "- **Input gate**: Important for selective memory updates\n",
    "- **Output gate**: Controls information flow to hidden state\n",
    "\n",
    "#### 4. Practical Considerations\n",
    "- **GRU**: Good default choice (fewer parameters, competitive performance)\n",
    "- **LSTM**: Use for critical applications requiring maximum performance\n",
    "- **Vanilla RNN**: Only for short sequences or when computational resources are extremely limited\n",
    "\n",
    "### When to Use What?\n",
    "- **Short sequences (<20 steps)**: Vanilla RNN might suffice\n",
    "- **Medium sequences (20-100 steps)**: GRU is often optimal\n",
    "- **Long sequences (>100 steps)**: LSTM or consider Transformers\n",
    "- **Limited compute**: GRU over LSTM\n",
    "- **Maximum performance**: LSTM with careful tuning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
