{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ed9893",
   "metadata": {},
   "source": [
    "# Part 2.4: RNN Applications - Sentiment Analysis\n",
    "\n",
    "Real-world application of RNNs for text sentiment analysis with comprehensive preprocessing and evaluation.\n",
    "\n",
    "## Objective\n",
    "- Implement end-to-end sentiment analysis pipeline\n",
    "- Compare RNN architectures on real text data\n",
    "- Text preprocessing and tokenization\n",
    "- Evaluation metrics and baseline comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"RNN Sentiment Analysis Application\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic movie review dataset (simulating IMDB-style data)\n",
    "def create_movie_reviews_dataset(num_samples=5000):\n",
    "    \"\"\"Create synthetic movie review dataset\"\"\"\n",
    "    \n",
    "    positive_words = [\n",
    "        'excellent', 'amazing', 'fantastic', 'wonderful', 'brilliant', 'outstanding',\n",
    "        'superb', 'great', 'good', 'beautiful', 'perfect', 'love', 'awesome',\n",
    "        'incredible', 'impressive', 'remarkable', 'entertaining', 'enjoyable',\n",
    "        'spectacular', 'magnificent', 'marvelous', 'delightful', 'charming'\n",
    "    ]\n",
    "    \n",
    "    negative_words = [\n",
    "        'terrible', 'awful', 'horrible', 'bad', 'worst', 'disappointing',\n",
    "        'boring', 'dull', 'stupid', 'waste', 'pathetic', 'annoying',\n",
    "        'ridiculous', 'absurd', 'pointless', 'tedious', 'uninteresting',\n",
    "        'frustrating', 'confusing', 'poorly', 'badly', 'mediocre', 'weak'\n",
    "    ]\n",
    "    \n",
    "    neutral_words = [\n",
    "        'movie', 'film', 'actor', 'actress', 'director', 'plot', 'story',\n",
    "        'character', 'scene', 'dialogue', 'cinematography', 'soundtrack',\n",
    "        'performance', 'script', 'drama', 'comedy', 'action', 'thriller',\n",
    "        'romance', 'adventure', 'watch', 'see', 'think', 'feel', 'time'\n",
    "    ]\n",
    "    \n",
    "    review_templates = {\n",
    "        'positive': [\n",
    "            \"This movie was {} and {} with {} performances.\",\n",
    "            \"I {} this film! The {} was {} and the acting was {}.\",\n",
    "            \"What an {} movie! {} cinematography and {} storyline.\",\n",
    "            \"The {} was {} and the {} made it even more {}.\",\n",
    "            \"Absolutely {} film with {} direction and {} characters.\"\n",
    "        ],\n",
    "        'negative': [\n",
    "            \"This movie was {} and {} with {} acting.\",\n",
    "            \"I found this film {} and {}. The plot was {} and {}.\",\n",
    "            \"What a {} movie! {} direction and {} performances.\",\n",
    "            \"The {} was {} and the {} made it even more {}.\",\n",
    "            \"Absolutely {} film with {} writing and {} execution.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    reviews = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Choose sentiment\n",
    "        is_positive = np.random.choice([True, False])\n",
    "        label = 1 if is_positive else 0\n",
    "        \n",
    "        # Choose template\n",
    "        sentiment_type = 'positive' if is_positive else 'negative'\n",
    "        template = np.random.choice(review_templates[sentiment_type])\n",
    "        \n",
    "        # Fill template\n",
    "        if is_positive:\n",
    "            words_to_use = positive_words + neutral_words\n",
    "        else:\n",
    "            words_to_use = negative_words + neutral_words\n",
    "        \n",
    "        # Count placeholders\n",
    "        num_placeholders = template.count('{}')\n",
    "        chosen_words = np.random.choice(words_to_use, num_placeholders, replace=True)\n",
    "        \n",
    "        review = template.format(*chosen_words)\n",
    "        \n",
    "        # Add some noise and variation\n",
    "        if np.random.random() < 0.3:  # Add extra sentences\n",
    "            extra_word = np.random.choice(words_to_use)\n",
    "            extra_sentence = f\" The {extra_word} was really something.\"\n",
    "            review += extra_sentence\n",
    "        \n",
    "        reviews.append(review)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return reviews, labels\n",
    "\n",
    "# Create dataset\n",
    "reviews, labels = create_movie_reviews_dataset(5000)\n",
    "\n",
    "print(f\"Created {len(reviews)} movie reviews\")\n",
    "print(f\"Positive reviews: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"Negative reviews: {len(labels)-sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nSample reviews:\")\n",
    "for i in range(3):\n",
    "    sentiment = \"Positive\" if labels[i] == 1 else \"Negative\"\n",
    "    print(f\"{sentiment}: {reviews[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072894c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing and tokenization\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, vocab_size=10000, max_length=100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.vocab_built = False\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary from training texts\"\"\"\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            cleaned_text = self.clean_text(text)\n",
    "            words = cleaned_text.split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Get most common words\n",
    "        most_common = word_counts.most_common(self.vocab_size - 2)  # -2 for PAD and UNK\n",
    "        \n",
    "        # Build word mappings\n",
    "        for idx, (word, count) in enumerate(most_common, start=2):\n",
    "            self.word_to_idx[word] = idx\n",
    "            self.idx_to_word[idx] = word\n",
    "        \n",
    "        self.vocab_built = True\n",
    "        print(f\"Vocabulary built with {len(self.word_to_idx)} words\")\n",
    "        print(f\"Most common words: {[word for word, _ in most_common[:10]]}\")\n",
    "    \n",
    "    def text_to_sequence(self, text):\n",
    "        \"\"\"Convert text to sequence of indices\"\"\"\n",
    "        if not self.vocab_built:\n",
    "            raise ValueError(\"Vocabulary not built. Call build_vocab first.\")\n",
    "        \n",
    "        cleaned_text = self.clean_text(text)\n",
    "        words = cleaned_text.split()\n",
    "        \n",
    "        # Convert to indices\n",
    "        sequence = []\n",
    "        for word in words:\n",
    "            idx = self.word_to_idx.get(word, self.word_to_idx['<UNK>'])\n",
    "            sequence.append(idx)\n",
    "        \n",
    "        # Truncate or pad\n",
    "        if len(sequence) > self.max_length:\n",
    "            sequence = sequence[:self.max_length]\n",
    "        else:\n",
    "            sequence.extend([self.word_to_idx['<PAD>']] * (self.max_length - len(sequence)))\n",
    "        \n",
    "        return torch.LongTensor(sequence)\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        \"\"\"Convert sequences back to texts\"\"\"\n",
    "        texts = []\n",
    "        for seq in sequences:\n",
    "            words = []\n",
    "            for idx in seq:\n",
    "                word = self.idx_to_word.get(idx.item(), '<UNK>')\n",
    "                if word != '<PAD>':\n",
    "                    words.append(word)\n",
    "            texts.append(' '.join(words))\n",
    "        return texts\n",
    "\n",
    "# Split data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    reviews, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Further split training into train/validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "# Initialize preprocessor and build vocabulary\n",
    "preprocessor = TextPreprocessor(vocab_size=5000, max_length=50)\n",
    "preprocessor.build_vocab(train_texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "print(\"\\nConverting texts to sequences...\")\n",
    "train_sequences = torch.stack([preprocessor.text_to_sequence(text) for text in train_texts])\n",
    "val_sequences = torch.stack([preprocessor.text_to_sequence(text) for text in val_texts])\n",
    "test_sequences = torch.stack([preprocessor.text_to_sequence(text) for text in test_texts])\n",
    "\n",
    "train_labels_tensor = torch.LongTensor(train_labels)\n",
    "val_labels_tensor = torch.LongTensor(val_labels)\n",
    "test_labels_tensor = torch.LongTensor(test_labels)\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "print(f\"Sequence shape: {train_sequences.shape}\")\n",
    "print(f\"Vocabulary size: {len(preprocessor.word_to_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a89d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for sentiment analysis\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = SentimentDataset(train_sequences, train_labels_tensor)\n",
    "val_dataset = SentimentDataset(val_sequences, val_labels_tensor)\n",
    "test_dataset = SentimentDataset(test_sequences, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Models for Sentiment Analysis\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, rnn_type='LSTM', \n",
    "                 num_layers=1, bidirectional=False, dropout=0.3):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # RNN layer\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, \n",
    "                              bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0,\n",
    "                              batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                             bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0,\n",
    "                             batch_first=True)\n",
    "        elif rnn_type == 'RNN':\n",
    "            self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                             bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0,\n",
    "                             batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        rnn_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(rnn_output_dim, output_dim)\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # RNN\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "        \n",
    "        # Use last output (for unidirectional) or concatenate last outputs (for bidirectional)\n",
    "        if self.bidirectional:\n",
    "            # Take last output from both directions\n",
    "            last_forward = rnn_output[:, -1, :rnn_output.size(2)//2]\n",
    "            last_backward = rnn_output[:, 0, rnn_output.size(2)//2:]\n",
    "            last_output = torch.cat([last_forward, last_backward], dim=1)\n",
    "        else:\n",
    "            last_output = rnn_output[:, -1, :]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.fc(self.dropout(last_output))\n",
    "        return output\n",
    "\n",
    "# Model configurations\n",
    "model_configs = {\n",
    "    'LSTM': {'rnn_type': 'LSTM', 'bidirectional': False},\n",
    "    'BiLSTM': {'rnn_type': 'LSTM', 'bidirectional': True},\n",
    "    'GRU': {'rnn_type': 'GRU', 'bidirectional': False},\n",
    "    'BiGRU': {'rnn_type': 'GRU', 'bidirectional': True},\n",
    "    'RNN': {'rnn_type': 'RNN', 'bidirectional': False},\n",
    "}\n",
    "\n",
    "print(\"RNN models defined!\")\n",
    "print(f\"Model configurations: {list(model_configs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12df055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_sentiment_model(model, train_loader, val_loader, num_epochs=20, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_acc = train_correct / total_train\n",
    "        val_acc = val_correct / total_val\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train all models\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Binary classification\n",
    "vocab_size = len(preprocessor.word_to_idx)\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\nüîÑ Training {model_name}...\")\n",
    "    \n",
    "    model = SentimentRNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        **config\n",
    "    ).to(device)\n",
    "    \n",
    "    history = train_sentiment_model(model, train_loader, val_loader, num_epochs=25)\n",
    "    \n",
    "    models[model_name] = model\n",
    "    histories[model_name] = history\n",
    "    \n",
    "    final_val_acc = history['val_acc'][-1]\n",
    "    print(f\"‚úÖ {model_name} completed - Final Val Accuracy: {final_val_acc:.3f}\")\n",
    "\n",
    "print(\"\\nüéâ All models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed639a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: TF-IDF + Logistic Regression\n",
    "print(\"\\nTraining baseline model (TF-IDF + Logistic Regression)...\")\n",
    "\n",
    "# TF-IDF feature extraction\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(train_texts)\n",
    "X_val_tfidf = tfidf.transform(val_texts)\n",
    "X_test_tfidf = tfidf.transform(test_texts)\n",
    "\n",
    "# Logistic Regression\n",
    "lr_baseline = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_baseline.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Baseline predictions\n",
    "val_pred_baseline = lr_baseline.predict(X_val_tfidf)\n",
    "baseline_val_acc = accuracy_score(val_labels, val_pred_baseline)\n",
    "\n",
    "print(f\"Baseline validation accuracy: {baseline_val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation on test set\n",
    "def evaluate_model(model, test_loader, model_name):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            outputs = model(sequences)\n",
    "            predicted = outputs.argmax(1).cpu().numpy()\n",
    "            \n",
    "            predictions.extend(predicted)\n",
    "            true_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "test_results = {}\n",
    "\n",
    "print(\"\\nEvaluating models on test set...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    results = evaluate_model(model, test_loader, model_name)\n",
    "    test_results[model_name] = results\n",
    "    print(f\"{model_name}: Acc={results['accuracy']:.3f}, F1={results['f1']:.3f}\")\n",
    "\n",
    "# Evaluate baseline\n",
    "test_pred_baseline = lr_baseline.predict(X_test_tfidf)\n",
    "baseline_accuracy = accuracy_score(test_labels, test_pred_baseline)\n",
    "baseline_precision, baseline_recall, baseline_f1, _ = precision_recall_fscore_support(\n",
    "    test_labels, test_pred_baseline, average='binary')\n",
    "\n",
    "test_results['Baseline (TF-IDF+LR)'] = {\n",
    "    'accuracy': baseline_accuracy,\n",
    "    'precision': baseline_precision,\n",
    "    'recall': baseline_recall,\n",
    "    'f1': baseline_f1,\n",
    "    'predictions': test_pred_baseline,\n",
    "    'true_labels': test_labels\n",
    "}\n",
    "\n",
    "print(f\"Baseline: Acc={baseline_accuracy:.3f}, F1={baseline_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('RNN Sentiment Analysis Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(model_configs)))\n",
    "model_colors = dict(zip(model_configs.keys(), colors))\n",
    "\n",
    "# 1. Training curves - Validation Accuracy\n",
    "ax = axes[0, 0]\n",
    "for model_name, history in histories.items():\n",
    "    ax.plot(history['val_acc'], color=model_colors[model_name], \n",
    "           linewidth=2, label=model_name)\n",
    "\n",
    "ax.axhline(y=baseline_val_acc, color='red', linestyle='--', \n",
    "          linewidth=2, label='Baseline (TF-IDF)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Accuracy')\n",
    "ax.set_title('Learning Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Test Performance Comparison\n",
    "ax = axes[0, 1]\n",
    "model_names = list(test_results.keys())\n",
    "accuracies = [test_results[name]['accuracy'] for name in model_names]\n",
    "f1_scores = [test_results[name]['f1'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Test Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 3. Confusion Matrix for Best Model\n",
    "best_model_name = max(test_results.keys(), key=lambda x: test_results[x]['f1'])\n",
    "best_predictions = test_results[best_model_name]['predictions']\n",
    "best_true_labels = test_results[best_model_name]['true_labels']\n",
    "\n",
    "ax = axes[0, 2]\n",
    "cm = confusion_matrix(best_true_labels, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title(f'Confusion Matrix - {best_model_name}')\n",
    "\n",
    "# 4. Training Loss Curves\n",
    "ax = axes[1, 0]\n",
    "for model_name, history in histories.items():\n",
    "    ax.plot(history['train_loss'], color=model_colors[model_name], \n",
    "           linewidth=2, label=f'{model_name} (Train)', alpha=0.7)\n",
    "    ax.plot(history['val_loss'], color=model_colors[model_name], \n",
    "           linewidth=2, linestyle='--', label=f'{model_name} (Val)')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training and Validation Loss')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Model Complexity Analysis\n",
    "ax = axes[1, 1]\n",
    "param_counts = []\n",
    "for model_name in model_configs.keys():\n",
    "    model = models[model_name]\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    param_counts.append(num_params)\n",
    "\n",
    "rnn_accuracies = [test_results[name]['accuracy'] for name in model_configs.keys()]\n",
    "\n",
    "scatter = ax.scatter(param_counts, rnn_accuracies, c=colors, s=150, alpha=0.7)\n",
    "for i, model_name in enumerate(model_configs.keys()):\n",
    "    ax.annotate(model_name, (param_counts[i], rnn_accuracies[i]), \n",
    "               xytext=(5, 5), textcoords='offset points', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Number of Parameters')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Model Complexity vs Performance')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Performance Summary Table\n",
    "ax = axes[1, 2]\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = []\n",
    "for model_name in model_names:\n",
    "    result = test_results[model_name]\n",
    "    table_data.append([\n",
    "        model_name,\n",
    "        f\"{result['accuracy']:.3f}\",\n",
    "        f\"{result['precision']:.3f}\",\n",
    "        f\"{result['recall']:.3f}\",\n",
    "        f\"{result['f1']:.3f}\"\n",
    "    ])\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                colLabels=['Model', 'Accuracy', 'Precision', 'Recall', 'F1'],\n",
    "                cellLoc='center',\n",
    "                loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 2)\n",
    "ax.set_title('Detailed Performance Metrics', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction examples and error analysis\n",
    "def analyze_predictions(model, preprocessor, test_texts, test_labels, model_name, num_examples=10):\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\nüîç PREDICTION ANALYSIS - {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    correct_examples = []\n",
    "    incorrect_examples = []\n",
    "    \n",
    "    for i, (text, true_label) in enumerate(zip(test_texts, test_labels)):\n",
    "        sequence = preprocessor.text_to_sequence(text).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(sequence)\n",
    "            predicted_label = output.argmax(1).item()\n",
    "            confidence = torch.softmax(output, dim=1).max().item()\n",
    "        \n",
    "        example = {\n",
    "            'text': text,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "        \n",
    "        if predicted_label == true_label:\n",
    "            correct_examples.append(example)\n",
    "        else:\n",
    "            incorrect_examples.append(example)\n",
    "    \n",
    "    # Show high-confidence correct predictions\n",
    "    print(\"\\n‚úÖ HIGH-CONFIDENCE CORRECT PREDICTIONS:\")\n",
    "    correct_examples_sorted = sorted(correct_examples, key=lambda x: x['confidence'], reverse=True)\n",
    "    for i, ex in enumerate(correct_examples_sorted[:3]):\n",
    "        sentiment = \"Positive\" if ex['predicted_label'] == 1 else \"Negative\"\n",
    "        print(f\"{i+1}. [{sentiment}] (Confidence: {ex['confidence']:.3f})\")\n",
    "        print(f\"   Text: {ex['text']}\")\n",
    "        print()\n",
    "    \n",
    "    # Show high-confidence incorrect predictions\n",
    "    print(\"‚ùå HIGH-CONFIDENCE INCORRECT PREDICTIONS:\")\n",
    "    incorrect_examples_sorted = sorted(incorrect_examples, key=lambda x: x['confidence'], reverse=True)\n",
    "    for i, ex in enumerate(incorrect_examples_sorted[:3]):\n",
    "        true_sentiment = \"Positive\" if ex['true_label'] == 1 else \"Negative\"\n",
    "        pred_sentiment = \"Positive\" if ex['predicted_label'] == 1 else \"Negative\"\n",
    "        print(f\"{i+1}. Predicted: [{pred_sentiment}], True: [{true_sentiment}] (Confidence: {ex['confidence']:.3f})\")\n",
    "        print(f\"   Text: {ex['text']}\")\n",
    "        print()\n",
    "\n",
    "# Analyze predictions for the best model\n",
    "best_model = models[best_model_name]\n",
    "analyze_predictions(best_model, preprocessor, test_texts, test_labels, best_model_name)\n",
    "\n",
    "# Also show some examples with the baseline\n",
    "print(f\"\\nüîç BASELINE PREDICTIONS (TF-IDF + Logistic Regression)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_correct = []\n",
    "baseline_incorrect = []\n",
    "\n",
    "for i, (text, true_label, pred_label) in enumerate(zip(test_texts, test_labels, test_pred_baseline)):\n",
    "    if pred_label == true_label:\n",
    "        baseline_correct.append((text, true_label, pred_label))\n",
    "    else:\n",
    "        baseline_incorrect.append((text, true_label, pred_label))\n",
    "\n",
    "print(\"\\n‚úÖ CORRECT BASELINE PREDICTIONS:\")\n",
    "for i, (text, true_label, pred_label) in enumerate(baseline_correct[:3]):\n",
    "    sentiment = \"Positive\" if pred_label == 1 else \"Negative\"\n",
    "    print(f\"{i+1}. [{sentiment}] - {text}\")\n",
    "\n",
    "print(\"\\n‚ùå INCORRECT BASELINE PREDICTIONS:\")\n",
    "for i, (text, true_label, pred_label) in enumerate(baseline_incorrect[:3]):\n",
    "    true_sentiment = \"Positive\" if true_label == 1 else \"Negative\"\n",
    "    pred_sentiment = \"Positive\" if pred_label == 1 else \"Negative\"\n",
    "    print(f\"{i+1}. Predicted: [{pred_sentiment}], True: [{true_sentiment}]\")\n",
    "    print(f\"   Text: {text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b82316",
   "metadata": {},
   "source": [
    "## RNN Sentiment Analysis - Comprehensive Analysis\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "#### 1. Model Performance Comparison\n",
    "- **Bidirectional models** generally outperform unidirectional versions\n",
    "- **LSTM/GRU** significantly better than vanilla RNN for this task\n",
    "- **BiLSTM** typically achieves best performance due to context from both directions\n",
    "\n",
    "#### 2. Architecture Advantages\n",
    "\n",
    "**Bidirectional RNNs**:\n",
    "- Access to future context improves sentiment understanding\n",
    "- Better at capturing negation and context-dependent sentiment\n",
    "- Higher computational cost but improved accuracy\n",
    "\n",
    "**LSTM vs GRU**:\n",
    "- LSTM: Better for complex, long-range dependencies\n",
    "- GRU: More efficient, competitive performance on shorter texts\n",
    "- Both significantly outperform vanilla RNN\n",
    "\n",
    "#### 3. Comparison with Traditional ML\n",
    "- **Deep learning advantage**: Better handling of context and word order\n",
    "- **TF-IDF baseline**: Competitive on bag-of-words features\n",
    "- **RNN benefit**: Sequential processing captures sentiment flow\n",
    "\n",
    "#### 4. Implementation Insights\n",
    "\n",
    "**Text Preprocessing**:\n",
    "- Vocabulary size: Balance between coverage and efficiency\n",
    "- Sequence length: Shorter sequences for efficiency, longer for context\n",
    "- Padding: Consistent sequence lengths for batch processing\n",
    "\n",
    "**Training Considerations**:\n",
    "- Gradient clipping: Essential for RNN training stability\n",
    "- Dropout: Prevents overfitting in embedding and RNN layers\n",
    "- Learning rate: Lower rates often better for RNN convergence\n",
    "\n",
    "#### 5. Error Analysis\n",
    "- **Common errors**: Sarcasm, negation, context-dependent sentiment\n",
    "- **Model confidence**: High confidence doesn't always mean correctness\n",
    "- **Baseline comparison**: RNNs better at complex linguistic patterns\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "1. **Social Media Monitoring**: Real-time sentiment analysis\n",
    "2. **Product Reviews**: Customer feedback analysis\n",
    "3. **Financial News**: Market sentiment tracking\n",
    "4. **Content Moderation**: Detecting negative content\n",
    "\n",
    "### Best Practices for RNN Sentiment Analysis\n",
    "\n",
    "1. **Choose BiLSTM** for best accuracy when computational resources allow\n",
    "2. **Use GRU** for faster training with competitive performance\n",
    "3. **Implement proper text preprocessing** pipeline\n",
    "4. **Apply gradient clipping** to prevent exploding gradients\n",
    "5. **Use early stopping** based on validation performance\n",
    "6. **Compare with simpler baselines** to validate deep learning benefit"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
