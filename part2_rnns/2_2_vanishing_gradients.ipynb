{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76503650",
   "metadata": {},
   "source": [
    "# Part 2.2: Vanishing Gradients in RNNs\n",
    "\n",
    "This notebook demonstrates the vanishing gradient problem in RNNs through experimental analysis.\n",
    "\n",
    "## Objective\n",
    "- Use long-sequence memory task (copy task)\n",
    "- Train vanilla RNN with sequence lengths 20, 50, and 100\n",
    "- Track gradient magnitudes at different time steps\n",
    "- Analyze spectral radius of Whh and gradient decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95106c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Vanishing Gradient Analysis in RNNs\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3631e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Task Dataset\n",
    "class CopyTaskDataset(Dataset):\n",
    "    def __init__(self, sequence_length, num_samples=1000, vocab_size=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_samples = num_samples\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Generate data\n",
    "        self.data = self._generate_data()\n",
    "        \n",
    "    def _generate_data(self):\n",
    "        data = []\n",
    "        for _ in range(self.num_samples):\n",
    "            # Random sequence\n",
    "            seq = torch.randint(1, self.vocab_size, (self.sequence_length,))\n",
    "            # Input: [sequence, zeros, delimiter]\n",
    "            delimiter = torch.tensor([self.vocab_size])  # Special delimiter token\n",
    "            zeros = torch.zeros(self.sequence_length, dtype=torch.long)\n",
    "            \n",
    "            input_seq = torch.cat([seq, zeros, delimiter])\n",
    "            target_seq = torch.cat([zeros, seq, torch.tensor([0])])\n",
    "            \n",
    "            data.append((input_seq, target_seq))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Vanilla RNN with gradient tracking\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers=1):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size + 1, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, \n",
    "                          batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size + 1)\n",
    "        \n",
    "        # Store gradients for analysis\n",
    "        self.gradient_norms = []\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def get_spectral_radius(self):\n",
    "        \"\"\"Calculate spectral radius of hidden-to-hidden weight matrix\"\"\"\n",
    "        Whh = self.rnn.weight_hh_l0.data.cpu().numpy()\n",
    "        eigenvalues = np.linalg.eigvals(Whh)\n",
    "        return np.max(np.abs(eigenvalues))\n",
    "\n",
    "print(\"Copy task and RNN classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73721b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with gradient tracking\n",
    "def train_and_track_gradients(model, dataloader, num_epochs=100):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    losses = []\n",
    "    gradient_norms_per_timestep = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_gradients = []\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, _ = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            # Track gradient norms at different timesteps\n",
    "            if epoch % 20 == 0:  # Sample every 20 epochs\n",
    "                timestep_grads = []\n",
    "                \n",
    "                # Approximate gradient flow by looking at parameter gradients\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'weight_hh' in name and param.grad is not None:\n",
    "                        grad_norm = param.grad.norm().item()\n",
    "                        timestep_grads.append(grad_norm)\n",
    "                \n",
    "                epoch_gradients.append(timestep_grads)\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        losses.append(epoch_loss / len(dataloader))\n",
    "        \n",
    "        if epoch_gradients:\n",
    "            gradient_norms_per_timestep.append(epoch_gradients)\n",
    "        \n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f'Epoch {epoch+1}: Loss = {losses[-1]:.4f}')\n",
    "    \n",
    "    return losses, gradient_norms_per_timestep\n",
    "\n",
    "# Experiment with different sequence lengths\n",
    "sequence_lengths = [20, 50, 100]\n",
    "results = {}\n",
    "\n",
    "print(\"\\nStarting experiments with different sequence lengths...\")\n",
    "\n",
    "for seq_len in sequence_lengths:\n",
    "    print(f\"\\nðŸ”„ Training on sequence length: {seq_len}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = CopyTaskDataset(sequence_length=seq_len, num_samples=500)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Create model\n",
    "    model = VanillaRNN(vocab_size=10, hidden_size=64).to(device)\n",
    "    spectral_radius = model.get_spectral_radius()\n",
    "    \n",
    "    print(f\"Initial spectral radius: {spectral_radius:.3f}\")\n",
    "    \n",
    "    # Train and track\n",
    "    losses, gradients = train_and_track_gradients(model, dataloader, num_epochs=100)\n",
    "    \n",
    "    # Final spectral radius\n",
    "    final_spectral_radius = model.get_spectral_radius()\n",
    "    \n",
    "    results[seq_len] = {\n",
    "        'losses': losses,\n",
    "        'gradients': gradients,\n",
    "        'initial_spectral_radius': spectral_radius,\n",
    "        'final_spectral_radius': final_spectral_radius,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Completed. Final spectral radius: {final_spectral_radius:.3f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab49e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization and analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Vanishing Gradient Analysis in RNNs', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['blue', 'orange', 'green']\n",
    "\n",
    "# 1. Training Loss Curves\n",
    "ax = axes[0, 0]\n",
    "for i, seq_len in enumerate(sequence_lengths):\n",
    "    losses = results[seq_len]['losses']\n",
    "    ax.plot(losses, color=colors[i], linewidth=2, label=f'Length {seq_len}')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss vs Sequence Length')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# 2. Spectral Radius Analysis\n",
    "ax = axes[0, 1]\n",
    "initial_sr = [results[sl]['initial_spectral_radius'] for sl in sequence_lengths]\n",
    "final_sr = [results[sl]['final_spectral_radius'] for sl in sequence_lengths]\n",
    "\n",
    "x = np.arange(len(sequence_lengths))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, initial_sr, width, label='Initial', alpha=0.8)\n",
    "ax.bar(x + width/2, final_sr, width, label='Final', alpha=0.8)\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Stability Threshold')\n",
    "\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Spectral Radius')\n",
    "ax.set_title('Spectral Radius of Whh Matrix')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sequence_lengths)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Eigenvalue Visualization for longest sequence\n",
    "ax = axes[0, 2]\n",
    "model_100 = results[100]['model']\n",
    "Whh = model_100.rnn.weight_hh_l0.data.cpu().numpy()\n",
    "eigenvals = np.linalg.eigvals(Whh)\n",
    "\n",
    "ax.scatter(eigenvals.real, eigenvals.imag, alpha=0.7, s=60)\n",
    "circle = plt.Circle((0, 0), 1, fill=False, color='red', linestyle='--', linewidth=2)\n",
    "ax.add_patch(circle)\n",
    "\n",
    "ax.set_xlabel('Real Part')\n",
    "ax.set_ylabel('Imaginary Part')\n",
    "ax.set_title('Eigenvalues of Whh (Length 100)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# 4. Learning Performance vs Sequence Length\n",
    "ax = axes[1, 0]\n",
    "final_losses = [results[sl]['losses'][-1] for sl in sequence_lengths]\n",
    "\n",
    "bars = ax.bar(sequence_lengths, final_losses, color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Final Loss')\n",
    "ax.set_title('Learning Difficulty vs Sequence Length')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Add value labels\n",
    "for bar, loss in zip(bars, final_losses):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n",
    "           f'{loss:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 5. Gradient Flow Analysis\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Simulate gradient decay\n",
    "time_steps = np.arange(1, 101)\n",
    "for i, seq_len in enumerate(sequence_lengths):\n",
    "    sr = results[seq_len]['final_spectral_radius']\n",
    "    # Approximate gradient magnitude decay\n",
    "    grad_magnitude = np.power(sr, time_steps)\n",
    "    ax.plot(time_steps, grad_magnitude, color=colors[i], \n",
    "           linewidth=2, label=f'Length {seq_len} (Ï={sr:.2f})')\n",
    "\n",
    "ax.set_xlabel('Time Steps Back')\n",
    "ax.set_ylabel('Relative Gradient Magnitude')\n",
    "ax.set_title('Gradient Decay Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# 6. Summary Table\n",
    "ax = axes[1, 2]\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = []\n",
    "for seq_len in sequence_lengths:\n",
    "    result = results[seq_len]\n",
    "    table_data.append([\n",
    "        f'{seq_len}',\n",
    "        f\"{result['final_spectral_radius']:.3f}\",\n",
    "        f\"{result['losses'][-1]:.3f}\",\n",
    "        'Yes' if result['losses'][-1] > 1.0 else 'No'\n",
    "    ])\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                colLabels=['Seq Length', 'Spectral Radius', 'Final Loss', 'Vanishing?'],\n",
    "                cellLoc='center',\n",
    "                loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.2, 2)\n",
    "ax.set_title('Vanishing Gradient Summary', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5d914",
   "metadata": {},
   "source": [
    "## Analysis: Vanishing Gradient Problem in RNNs\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Sequence Length Impact**: Longer sequences show degraded learning performance\n",
    "2. **Spectral Radius**: Values close to or above 1.0 indicate potential instability\n",
    "3. **Gradient Decay**: Exponential decay of gradients over long sequences\n",
    "4. **Learning Failure**: Copy task becomes impossible for very long sequences\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The vanishing gradient problem stems from the recursive nature of RNN computation:\n",
    "- Gradients flow backward through matrix multiplications\n",
    "- Each step involves multiplication by the weight matrix Whh\n",
    "- If spectral radius < 1, gradients vanish exponentially\n",
    "- If spectral radius > 1, gradients explode exponentially\n",
    "\n",
    "### Solutions\n",
    "\n",
    "- **LSTM/GRU**: Gating mechanisms create gradient highways\n",
    "- **Gradient Clipping**: Prevents exploding gradients\n",
    "- **Careful Initialization**: Initialize weights to have spectral radius â‰ˆ 1\n",
    "- **Skip Connections**: Provide direct gradient paths"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
