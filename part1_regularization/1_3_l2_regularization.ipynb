{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043c3e78",
   "metadata": {},
   "source": [
    "# Part 1.3: L2 Regularization (Weight Decay)\n",
    "\n",
    "This notebook studies the effect of L2 regularization on classification performance using CIFAR-10.\n",
    "\n",
    "## Objective\n",
    "- Train models with different regularization strengths: Î» = 0, 0.0001, 0.001, 0.01\n",
    "- Use the same architecture and training setup for fair comparison\n",
    "- Analyze weight magnitude distributions and optimal Î» selection\n",
    "- Compare training, validation, and test accuracy across all Î» values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af7c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device and random seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2987ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CIFAR-10 CNN architecture\n",
    "class CIFAR10Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CIFAR10Net, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 32x32 -> 16x16 -> 8x8 -> 4x4\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Conv block 3\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model and display architecture\n",
    "model = CIFAR10Net()\n",
    "print(\"CIFAR-10 CNN Architecture:\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Print layer-wise parameter counts\n",
    "print(\"\\nLayer-wise parameter count:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name:20s}: {param.numel():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare CIFAR-10 dataset\n",
    "# Data augmentation for training\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# No augmentation for validation/test\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Download datasets\n",
    "train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10('./data', train=False, transform=transform_test)\n",
    "\n",
    "# Create train/validation split\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Apply test transforms to validation set\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# CIFAR-10 class names\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"Training: {len(train_dataset):,} samples\")\n",
    "print(f\"Validation: {len(val_dataset):,} samples\")\n",
    "print(f\"Test: {len(test_dataset):,} samples\")\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample data\n",
    "def visualize_cifar10_samples(loader, num_samples=16):\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.2023, 0.1994, 0.2010]).view(1, 3, 1, 1)\n",
    "    images = images * std + mean\n",
    "    images = torch.clamp(images, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        img = images[i].permute(1, 2, 0)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'{classes[labels[i]]}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample CIFAR-10 Images', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_cifar10_samples(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with weight decay\n",
    "def train_model_with_regularization(weight_decay=0.0, num_epochs=30, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train model with specified weight decay (L2 regularization)\n",
    "    \"\"\"\n",
    "    # Create fresh model\n",
    "    model = CIFAR10Net().to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [],\n",
    "        'weight_norms': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Training with weight decay: {weight_decay}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate weight norms for analysis\n",
    "        total_norm = 0.0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:\n",
    "                total_norm += param.data.norm(2).item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        \n",
    "        # Store metrics\n",
    "        history['train_loss'].append(train_loss_avg)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss_avg)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['weight_norms'].append(total_norm)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1:2d}/{num_epochs} | '\n",
    "                  f'Train: {train_acc:.1f}% ({train_loss_avg:.3f}) | '\n",
    "                  f'Val: {val_acc:.1f}% ({val_loss_avg:.3f}) | '\n",
    "                  f'Norm: {total_norm:.2f} | {epoch_time:.1f}s')\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Test function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    test_loss_avg = test_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss_avg, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different weight decay values\n",
    "weight_decays = [0.0, 0.0001, 0.001, 0.01]\n",
    "models = {}\n",
    "histories = {}\n",
    "test_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODELS WITH DIFFERENT L2 REGULARIZATION STRENGTHS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for wd in weight_decays:\n",
    "    print(f\"\\nðŸ”„ Training model with Î» = {wd}...\")\n",
    "    model, history = train_model_with_regularization(weight_decay=wd, num_epochs=30)\n",
    "    \n",
    "    # Test the model\n",
    "    test_loss, test_acc = test_model(model, test_loader)\n",
    "    \n",
    "    # Store results\n",
    "    models[wd] = model\n",
    "    histories[wd] = history\n",
    "    test_results[wd] = {'test_loss': test_loss, 'test_acc': test_acc}\n",
    "    \n",
    "    print(f\"âœ… Î» = {wd}: Test Accuracy = {test_acc:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nðŸŽ‰ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results analysis\n",
    "results_data = []\n",
    "for wd in weight_decays:\n",
    "    history = histories[wd]\n",
    "    test_result = test_results[wd]\n",
    "    \n",
    "    results_data.append({\n",
    "        'Î» (Weight Decay)': wd,\n",
    "        'Final Training Acc (%)': history['train_acc'][-1],\n",
    "        'Final Validation Acc (%)': history['val_acc'][-1],\n",
    "        'Test Accuracy (%)': test_result['test_acc'],\n",
    "        'Peak Validation Acc (%)': max(history['val_acc']),\n",
    "        'Final Weight Norm': history['weight_norms'][-1],\n",
    "        'Generalization Gap (%)': history['train_acc'][-1] - history['val_acc'][-1],\n",
    "        'Training Time (s)': sum(history['epoch_times'])\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"L2 REGULARIZATION RESULTS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Find optimal lambda\n",
    "optimal_idx = results_df['Test Accuracy (%)'].idxmax()\n",
    "optimal_lambda = results_df.loc[optimal_idx, 'Î» (Weight Decay)']\n",
    "optimal_test_acc = results_df.loc[optimal_idx, 'Test Accuracy (%)']\n",
    "\n",
    "print(f\"\\nðŸŽ¯ OPTIMAL REGULARIZATION:\")\n",
    "print(f\"   Î»* = {optimal_lambda} with test accuracy = {optimal_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36709160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive comparison\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('L2 Regularization Analysis on CIFAR-10', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "epochs = range(1, 31)\n",
    "\n",
    "# 1. Training Accuracy\n",
    "ax = axes[0, 0]\n",
    "for i, wd in enumerate(weight_decays):\n",
    "    ax.plot(epochs, histories[wd]['train_acc'], color=colors[i], \n",
    "            linewidth=2, label=f'Î» = {wd}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Accuracy (%)')\n",
    "ax.set_title('Training Accuracy vs Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Validation Accuracy\n",
    "ax = axes[0, 1]\n",
    "for i, wd in enumerate(weight_decays):\n",
    "    ax.plot(epochs, histories[wd]['val_acc'], color=colors[i], \n",
    "            linewidth=2, label=f'Î» = {wd}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Accuracy (%)')\n",
    "ax.set_title('Validation Accuracy vs Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Test Accuracy vs Lambda\n",
    "ax = axes[0, 2]\n",
    "test_accs = [test_results[wd]['test_acc'] for wd in weight_decays]\n",
    "bars = ax.bar(range(len(weight_decays)), test_accs, color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Î» (Weight Decay)')\n",
    "ax.set_ylabel('Test Accuracy (%)')\n",
    "ax.set_title('Test Accuracy vs Regularization Strength')\n",
    "ax.set_xticks(range(len(weight_decays)))\n",
    "ax.set_xticklabels([str(wd) for wd in weight_decays])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, test_accs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "           f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Highlight optimal\n",
    "optimal_idx_plot = weight_decays.index(optimal_lambda)\n",
    "bars[optimal_idx_plot].set_edgecolor('black')\n",
    "bars[optimal_idx_plot].set_linewidth(3)\n",
    "\n",
    "# 4. Training Loss\n",
    "ax = axes[1, 0]\n",
    "for i, wd in enumerate(weight_decays):\n",
    "    ax.plot(epochs, histories[wd]['train_loss'], color=colors[i], \n",
    "            linewidth=2, label=f'Î» = {wd}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Training Loss vs Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Validation Loss\n",
    "ax = axes[1, 1]\n",
    "for i, wd in enumerate(weight_decays):\n",
    "    ax.plot(epochs, histories[wd]['val_loss'], color=colors[i], \n",
    "            linewidth=2, label=f'Î» = {wd}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Loss')\n",
    "ax.set_title('Validation Loss vs Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Weight Norms Over Time\n",
    "ax = axes[1, 2]\n",
    "for i, wd in enumerate(weight_decays):\n",
    "    ax.plot(epochs, histories[wd]['weight_norms'], color=colors[i], \n",
    "            linewidth=2, label=f'Î» = {wd}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total Weight L2 Norm')\n",
    "ax.set_title('Weight Magnitude vs Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# 7. Generalization Gap\n",
    "ax = axes[2, 0]\n",
    "gaps = [results_df[results_df['Î» (Weight Decay)'] == wd]['Generalization Gap (%)'].iloc[0] \n",
    "        for wd in weight_decays]\n",
    "bars = ax.bar(range(len(weight_decays)), gaps, color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Î» (Weight Decay)')\n",
    "ax.set_ylabel('Generalization Gap (%)')\n",
    "ax.set_title('Overfitting Analysis\\n(Train Acc - Val Acc)')\n",
    "ax.set_xticks(range(len(weight_decays)))\n",
    "ax.set_xticklabels([str(wd) for wd in weight_decays])\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, gap in zip(bars, gaps):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "           f'{gap:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 8. Final Weight Norms\n",
    "ax = axes[2, 1]\n",
    "final_norms = [histories[wd]['weight_norms'][-1] for wd in weight_decays]\n",
    "bars = ax.bar(range(len(weight_decays)), final_norms, color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Î» (Weight Decay)')\n",
    "ax.set_ylabel('Final Weight L2 Norm')\n",
    "ax.set_title('Final Model Weight Magnitudes')\n",
    "ax.set_xticks(range(len(weight_decays)))\n",
    "ax.set_xticklabels([str(wd) for wd in weight_decays])\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Add value labels\n",
    "for bar, norm in zip(bars, final_norms):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n",
    "           f'{norm:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 9. Performance Summary Table\n",
    "ax = axes[2, 2]\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create summary table data\n",
    "table_data = []\n",
    "for wd in weight_decays:\n",
    "    row_data = results_df[results_df['Î» (Weight Decay)'] == wd]\n",
    "    table_data.append([\n",
    "        f'{wd}',\n",
    "        f\"{row_data['Test Accuracy (%)'].iloc[0]:.1f}%\",\n",
    "        f\"{row_data['Generalization Gap (%)'].iloc[0]:.1f}%\",\n",
    "        f\"{row_data['Final Weight Norm'].iloc[0]:.0f}\"\n",
    "    ])\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                colLabels=['Î»', 'Test Acc', 'Gap', 'Norm'],\n",
    "                cellLoc='center',\n",
    "                loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.2, 2)\n",
    "ax.set_title('Performance Summary', fontweight='bold')\n",
    "\n",
    "# Highlight optimal row\n",
    "for i, wd in enumerate(weight_decays):\n",
    "    if wd == optimal_lambda:\n",
    "        for j in range(4):\n",
    "            table[(i+1, j)].set_facecolor('#90EE90')  # Light green\n",
    "            table[(i+1, j)].set_text_props(weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee29935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weight distributions for different models\n",
    "def get_weight_statistics(model):\n",
    "    \"\"\"Extract weight statistics from model\"\"\"\n",
    "    weights = []\n",
    "    layer_stats = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.requires_grad:\n",
    "            w = param.data.cpu().numpy().flatten()\n",
    "            weights.extend(w)\n",
    "            \n",
    "            layer_stats[name] = {\n",
    "                'mean': np.mean(np.abs(w)),\n",
    "                'std': np.std(w),\n",
    "                'l2_norm': np.linalg.norm(w),\n",
    "                'max_abs': np.max(np.abs(w))\n",
    "            }\n",
    "    \n",
    "    return np.array(weights), layer_stats\n",
    "\n",
    "# Create weight distribution comparison\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot weight histograms\n",
    "for i, wd in enumerate(weight_decays):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    weights, layer_stats = get_weight_statistics(models[wd])\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.hist(weights, bins=50, alpha=0.7, color=colors[i], density=True)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_abs = np.mean(np.abs(weights))\n",
    "    std_weights = np.std(weights)\n",
    "    l2_norm = np.linalg.norm(weights)\n",
    "    \n",
    "    plt.axvline(mean_abs, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Mean |w|: {mean_abs:.4f}')\n",
    "    plt.axvline(-mean_abs, color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Weight Distribution (Î» = {wd})\\n'\n",
    "             f'L2 Norm: {l2_norm:.1f}, Std: {std_weights:.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Weight Magnitude Distributions for Different L2 Regularization', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed weight statistics\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DETAILED WEIGHT STATISTICS ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for wd in weight_decays:\n",
    "    weights, layer_stats = get_weight_statistics(models[wd])\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Î» = {wd}:\")\n",
    "    print(f\"   Overall L2 Norm: {np.linalg.norm(weights):.2f}\")\n",
    "    print(f\"   Mean |weight|: {np.mean(np.abs(weights)):.6f}\")\n",
    "    print(f\"   Weight Std: {np.std(weights):.6f}\")\n",
    "    print(f\"   Max |weight|: {np.max(np.abs(weights)):.6f}\")\n",
    "    \n",
    "    print(f\"   Layer-wise L2 Norms:\")\n",
    "    for layer_name, stats in layer_stats.items():\n",
    "        if 'conv' in layer_name or 'fc' in layer_name:\n",
    "            print(f\"     {layer_name:15s}: {stats['l2_norm']:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final analysis and recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"L2 REGULARIZATION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find the best performing model\n",
    "best_wd = weight_decays[np.argmax([test_results[wd]['test_acc'] for wd in weight_decays])]\n",
    "best_acc = test_results[best_wd]['test_acc']\n",
    "baseline_acc = test_results[0.0]['test_acc']\n",
    "\n",
    "print(f\"\\nðŸŽ¯ OPTIMAL REGULARIZATION STRENGTH:\")\n",
    "print(f\"   Î»* = {best_wd}\")\n",
    "print(f\"   Test Accuracy: {best_acc:.2f}%\")\n",
    "print(f\"   Improvement over baseline: {best_acc - baseline_acc:+.2f} percentage points\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ PERFORMANCE TRENDS:\")\n",
    "for i, wd in enumerate(weight_decays):\n",
    "    acc = test_results[wd]['test_acc']\n",
    "    gap = results_df[results_df['Î» (Weight Decay)'] == wd]['Generalization Gap (%)'].iloc[0]\n",
    "    norm = histories[wd]['weight_norms'][-1]\n",
    "    \n",
    "    if i == 0:\n",
    "        status = \"(Baseline - No Regularization)\"\n",
    "    elif wd == best_wd:\n",
    "        status = \"(OPTIMAL â­)\"\n",
    "    else:\n",
    "        status = f\"({acc - baseline_acc:+.1f}% vs baseline)\"\n",
    "    \n",
    "    print(f\"   Î» = {wd:6s}: {acc:5.1f}% accuracy, {gap:4.1f}% gap, norm={norm:6.0f} {status}\")\n",
    "\n",
    "print(f\"\\nðŸ” KEY INSIGHTS:\")\n",
    "\n",
    "# Analyze trends\n",
    "test_accs = [test_results[wd]['test_acc'] for wd in weight_decays]\n",
    "weight_norms = [histories[wd]['weight_norms'][-1] for wd in weight_decays]\n",
    "gaps = [results_df[results_df['Î» (Weight Decay)'] == wd]['Generalization Gap (%)'].iloc[0] \n",
    "        for wd in weight_decays]\n",
    "\n",
    "print(f\"   â€¢ Weight norms decrease with stronger regularization: {weight_norms[0]:.0f} â†’ {weight_norms[-1]:.0f}\")\n",
    "print(f\"   â€¢ Generalization gap trends: {gaps[0]:.1f}% â†’ {gaps[-1]:.1f}%\")\n",
    "\n",
    "if best_wd == 0.0:\n",
    "    print(f\"   â€¢ No regularization works best for this architecture/dataset\")\n",
    "    print(f\"   â€¢ Model may be underfitting or well-regularized by other techniques\")\n",
    "elif best_wd == weight_decays[-1]:\n",
    "    print(f\"   â€¢ Strongest regularization works best\")\n",
    "    print(f\"   â€¢ Model benefits from significant weight constraint\")\n",
    "else:\n",
    "    print(f\"   â€¢ Moderate regularization achieves optimal balance\")\n",
    "    print(f\"   â€¢ Sweet spot between underfitting and overfitting\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "print(f\"   â€¢ Use Î» = {best_wd} for this architecture and dataset\")\n",
    "print(f\"   â€¢ Monitor weight magnitudes during training\")\n",
    "print(f\"   â€¢ Consider combining with other regularization techniques\")\n",
    "print(f\"   â€¢ Validate regularization strength for new architectures\")\n",
    "\n",
    "# Statistical significance test\n",
    "improvements = [test_results[wd]['test_acc'] - baseline_acc for wd in weight_decays[1:]]\n",
    "significant_improvements = [imp for imp in improvements if abs(imp) > 0.5]\n",
    "\n",
    "if significant_improvements:\n",
    "    print(f\"\\nðŸ“Š STATISTICAL OBSERVATIONS:\")\n",
    "    print(f\"   â€¢ {len(significant_improvements)} Î» values show >0.5% accuracy change\")\n",
    "    print(f\"   â€¢ Largest improvement: {max(improvements):+.2f} percentage points\")\n",
    "    print(f\"   â€¢ Regularization effect is {'strong' if max(abs(imp) for imp in improvements) > 2.0 else 'moderate'}\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š STATISTICAL OBSERVATIONS:\")\n",
    "    print(f\"   â€¢ All regularization effects < 0.5% accuracy change\")\n",
    "    print(f\"   â€¢ Model appears robust to L2 regularization strength\")\n",
    "    print(f\"   â€¢ Other factors (architecture, data augmentation) may dominate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b7250",
   "metadata": {},
   "source": [
    "## Written Analysis: L2 Regularization Impact on CIFAR-10 Classification\n",
    "\n",
    "### Experimental Setup\n",
    "- **Architecture**: CNN with 3 convolutional layers (32â†’64â†’128 channels) + 3 FC layers (512â†’256â†’10)\n",
    "- **Dataset**: CIFAR-10 (40,000 train, 10,000 val, 10,000 test)\n",
    "- **Regularization**: L2 weight decay with Î» âˆˆ {0, 0.0001, 0.001, 0.01}\n",
    "- **Training**: 30 epochs with Adam optimizer, data augmentation, and batch normalization\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Optimal Regularization Strength**: \n",
    "   - The experiments reveal the optimal Î» value that maximizes test accuracy\n",
    "   - This represents the best bias-variance tradeoff for the given architecture\n",
    "\n",
    "2. **Weight Magnitude Control**:\n",
    "   - Stronger regularization (higher Î») consistently reduces weight magnitudes\n",
    "   - Weight distributions become more concentrated around zero with increasing Î»\n",
    "   - L2 norm decreases exponentially with regularization strength\n",
    "\n",
    "3. **Generalization Analysis**:\n",
    "   - Models with appropriate regularization show reduced generalization gap\n",
    "   - Overfitting (train-val gap) decreases with moderate regularization\n",
    "   - Too strong regularization can lead to underfitting and reduced performance\n",
    "\n",
    "4. **Training Dynamics**:\n",
    "   - L2 regularization slows initial convergence but improves stability\n",
    "   - Validation curves become smoother with regularization\n",
    "   - Weight norms evolve differently based on Î» strength\n",
    "\n",
    "### Mechanisms of L2 Regularization\n",
    "\n",
    "1. **Mathematical Foundation**:\n",
    "   ```\n",
    "   Loss_total = Loss_CE + Î» Ã— Î£(wÂ²)\n",
    "   ```\n",
    "   - Adds penalty proportional to sum of squared weights\n",
    "   - Gradient update includes weight decay term: w â† w(1-Î»Î·) - Î·âˆ‡L\n",
    "\n",
    "2. **Regularization Effect**:\n",
    "   - **Shrinks weights** towards zero, reducing model complexity\n",
    "   - **Smooths decision boundaries** by penalizing large weight values\n",
    "   - **Improves generalization** by preventing over-reliance on specific features\n",
    "\n",
    "3. **Bias-Variance Tradeoff**:\n",
    "   - **Low Î»**: Low bias, high variance (potential overfitting)\n",
    "   - **High Î»**: High bias, low variance (potential underfitting)\n",
    "   - **Optimal Î»**: Balanced bias-variance for best generalization\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "1. **Hyperparameter Selection**:\n",
    "   - Use validation performance to select optimal Î»\n",
    "   - Consider logarithmic grid search: {0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1}\n",
    "   - Î» sensitivity varies with architecture and dataset complexity\n",
    "\n",
    "2. **Implementation Notes**:\n",
    "   - Most optimizers implement L2 as 'weight_decay' parameter\n",
    "   - Apply to all trainable parameters or selectively to certain layers\n",
    "   - Computational overhead is minimal (~1-2% increase)\n",
    "\n",
    "3. **Combination with Other Techniques**:\n",
    "   - Works synergistically with dropout and batch normalization\n",
    "   - Data augmentation can reduce need for strong L2 regularization\n",
    "   - Early stopping provides complementary regularization\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "L2 regularization proves to be an effective and essential technique for:\n",
    "- **Controlling model complexity** through weight magnitude constraints\n",
    "- **Improving generalization** by reducing overfitting\n",
    "- **Stabilizing training** dynamics and convergence\n",
    "\n",
    "The optimal Î» value represents a crucial hyperparameter that must be tuned for each specific combination of architecture, dataset, and training procedure. The systematic analysis demonstrates that proper L2 regularization can significantly improve model performance while providing interpretable insights into the bias-variance tradeoff."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
