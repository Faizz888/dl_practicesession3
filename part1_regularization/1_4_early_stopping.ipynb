{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0abea0f",
   "metadata": {},
   "source": [
    "# Part 1.4: Early Stopping as Regularization\n",
    "\n",
    "This notebook demonstrates early stopping as a regularization technique on image classification.\n",
    "\n",
    "## Objective\n",
    "- Train one model for fixed 50 epochs without early stopping\n",
    "- Train second model with early stopping (patience 5-10)\n",
    "- Compare final test accuracy and training efficiency\n",
    "- Analyze computational savings and performance impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Set device and random seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class EarlyStoppingNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(EarlyStoppingNet, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers - MNIST 28x28 -> 14x14 -> 7x7 -> 3x3 after pooling\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv blocks with pooling\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        \n",
    "        # FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model and display info\n",
    "model = EarlyStoppingNet()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23251d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))\n",
    "])\n",
    "\n",
    "# Download datasets\n",
    "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.FashionMNIST('./data', train=False, transform=transform_test)\n",
    "\n",
    "# Create train/validation split\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Apply test transforms to validation\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Fashion-MNIST class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"Training: {len(train_dataset):,} samples\")\n",
    "print(f\"Validation: {len(val_dataset):,} samples\")\n",
    "print(f\"Test: {len(test_dataset):,} samples\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae804527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample data\n",
    "def visualize_fashion_mnist(loader, num_samples=12):\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(3, 4, i+1)\n",
    "        # Denormalize for visualization\n",
    "        img = images[i].squeeze() * 0.3530 + 0.2860\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f'{class_names[labels[i]]}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Fashion-MNIST Images', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_fashion_mnist(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493cec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping implementation\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility class\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_score, model):\n",
    "        score = val_score\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "            \n",
    "        return self.early_stop\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Save best model weights\"\"\"\n",
    "        if self.restore_best_weights:\n",
    "            self.best_weights = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    def restore_best(self, model):\n",
    "        \"\"\"Restore best model weights\"\"\"\n",
    "        if self.restore_best_weights and self.best_weights is not None:\n",
    "            model.load_state_dict(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1527a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_fixed_epochs(num_epochs=50):\n",
    "    \"\"\"Train model for fixed number of epochs\"\"\"\n",
    "    model = EarlyStoppingNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'times': []}\n",
    "    \n",
    "    print(f\"Training for FIXED {num_epochs} epochs...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        history['train_loss'].append(train_loss_avg)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss_avg)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['times'].append(epoch_time)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1:2d}/{num_epochs} | '\n",
    "                  f'Train: {train_acc:.1f}% ({train_loss_avg:.3f}) | '\n",
    "                  f'Val: {val_acc:.1f}% ({val_loss_avg:.3f}) | '\n",
    "                  f'{epoch_time:.1f}s')\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\nCompleted all {num_epochs} epochs in {total_time:.1f}s\")\n",
    "    \n",
    "    return model, history, total_time\n",
    "\n",
    "def train_with_early_stopping(patience=7, max_epochs=50):\n",
    "    \"\"\"Train model with early stopping\"\"\"\n",
    "    model = EarlyStoppingNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'times': []}\n",
    "    \n",
    "    print(f\"Training with EARLY STOPPING (patience={patience})...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        history['train_loss'].append(train_loss_avg)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss_avg)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['times'].append(epoch_time)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Check early stopping\n",
    "        if early_stopping(val_acc, model):\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            print(f\"Best validation accuracy: {early_stopping.best_score:.2f}%\")\n",
    "            early_stopping.restore_best(model)\n",
    "            break\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1:2d}/{max_epochs} | '\n",
    "                  f'Train: {train_acc:.1f}% ({train_loss_avg:.3f}) | '\n",
    "                  f'Val: {val_acc:.1f}% ({val_loss_avg:.3f}) | '\n",
    "                  f'Patience: {early_stopping.counter}/{patience} | {epoch_time:.1f}s')\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    actual_epochs = len(history['train_loss'])\n",
    "    \n",
    "    print(f\"\\nCompleted training in {actual_epochs} epochs ({total_time:.1f}s)\")\n",
    "    print(f\"Early stopping point marked at epoch {actual_epochs}\")\n",
    "    \n",
    "    return model, history, total_time, actual_epochs\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    test_loss_avg = test_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss_avg, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0be422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODEL WITHOUT EARLY STOPPING (50 epochs)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_fixed, history_fixed, time_fixed = train_fixed_epochs(num_epochs=50)\n",
    "test_loss_fixed, test_acc_fixed = test_model(model_fixed, test_loader)\n",
    "\n",
    "print(f\"\\nFixed training results:\")\n",
    "print(f\"Test accuracy: {test_acc_fixed:.2f}%\")\n",
    "print(f\"Total training time: {time_fixed:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODEL WITH EARLY STOPPING (patience=7)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_early, history_early, time_early, epochs_early = train_with_early_stopping(patience=7, max_epochs=50)\n",
    "test_loss_early, test_acc_early = test_model(model_early, test_loader)\n",
    "\n",
    "print(f\"\\nEarly stopping results:\")\n",
    "print(f\"Test accuracy: {test_acc_early:.2f}%\")\n",
    "print(f\"Epochs trained: {epochs_early}\")\n",
    "print(f\"Total training time: {time_early:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plots\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Training and Validation Loss\n",
    "plt.subplot(2, 3, 1)\n",
    "epochs_fixed = range(1, len(history_fixed['train_loss']) + 1)\n",
    "epochs_early = range(1, len(history_early['train_loss']) + 1)\n",
    "\n",
    "plt.plot(epochs_fixed, history_fixed['train_loss'], 'b-', linewidth=2, label='Fixed - Train', alpha=0.7)\n",
    "plt.plot(epochs_fixed, history_fixed['val_loss'], 'b--', linewidth=2, label='Fixed - Val')\n",
    "plt.plot(epochs_early, history_early['train_loss'], 'r-', linewidth=2, label='Early Stop - Train', alpha=0.7)\n",
    "plt.plot(epochs_early, history_early['val_loss'], 'r--', linewidth=2, label='Early Stop - Val')\n",
    "\n",
    "# Mark early stopping point\n",
    "plt.axvline(x=epochs_early[-1], color='red', linestyle=':', linewidth=3, alpha=0.8, label='Early Stop Point')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training and Validation Accuracy\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(epochs_fixed, history_fixed['train_acc'], 'b-', linewidth=2, label='Fixed - Train', alpha=0.7)\n",
    "plt.plot(epochs_fixed, history_fixed['val_acc'], 'b--', linewidth=2, label='Fixed - Val')\n",
    "plt.plot(epochs_early, history_early['train_acc'], 'r-', linewidth=2, label='Early Stop - Train', alpha=0.7)\n",
    "plt.plot(epochs_early, history_early['val_acc'], 'r--', linewidth=2, label='Early Stop - Val')\n",
    "\n",
    "# Mark early stopping point\n",
    "plt.axvline(x=epochs_early[-1], color='red', linestyle=':', linewidth=3, alpha=0.8, label='Early Stop Point')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Final Performance Comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "models = ['Fixed\\n50 epochs', 'Early Stopping\\n{} epochs'.format(epochs_early)]\n",
    "test_accs = [test_acc_fixed, test_acc_early]\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "bars = plt.bar(models, test_accs, color=colors, alpha=0.7)\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Final Test Accuracy Comparison', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, test_accs):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.3,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 4. Training Time Comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "times = [time_fixed, time_early]\n",
    "bars = plt.bar(models, times, color=colors, alpha=0.7)\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Training Time Comparison', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels and savings\n",
    "for bar, time_val in zip(bars, times):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "            f'{time_val:.0f}s', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "time_saved = time_fixed - time_early\n",
    "percent_saved = (time_saved / time_fixed) * 100\n",
    "plt.text(0.5, max(times) * 0.7, f'Time Saved:\\n{time_saved:.0f}s ({percent_saved:.1f}%)',\n",
    "         ha='center', va='center', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "         fontsize=12, fontweight='bold')\n",
    "\n",
    "# 5. Generalization Gap Analysis\n",
    "plt.subplot(2, 3, 5)\n",
    "# Calculate generalization gap for both models\n",
    "gap_fixed = np.array(history_fixed['train_acc']) - np.array(history_fixed['val_acc'])\n",
    "gap_early = np.array(history_early['train_acc']) - np.array(history_early['val_acc'])\n",
    "\n",
    "plt.plot(epochs_fixed, gap_fixed, 'b-', linewidth=2, label='Fixed Training')\n",
    "plt.plot(epochs_early, gap_early, 'r-', linewidth=2, label='Early Stopping')\n",
    "plt.axvline(x=epochs_early[-1], color='red', linestyle=':', linewidth=3, alpha=0.8)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Generalization Gap (%)')\n",
    "plt.title('Overfitting Analysis\\n(Train Acc - Val Acc)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Efficiency Analysis\n",
    "plt.subplot(2, 3, 6)\n",
    "metrics = ['Epochs Used', 'Time Saved (%)', 'Accuracy Diff']\n",
    "values = [epochs_early, percent_saved, test_acc_early - test_acc_fixed]\n",
    "\n",
    "# Normalize for visualization\n",
    "normalized_values = [epochs_early/50*100, percent_saved, (test_acc_early - test_acc_fixed)*10 + 50]\n",
    "colors_bar = ['orange', 'green', 'purple']\n",
    "\n",
    "bars = plt.bar(metrics, normalized_values, color=colors_bar, alpha=0.7)\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.title('Efficiency Metrics\\n(Normalized)', fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add actual values as labels\n",
    "labels = [f'{epochs_early}/50', f'{percent_saved:.1f}%', f'{test_acc_early - test_acc_fixed:+.1f}%']\n",
    "for bar, label in zip(bars, labels):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            label, ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757eac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis and metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE EARLY STOPPING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate metrics\n",
    "time_saved = time_fixed - time_early\n",
    "percent_time_saved = (time_saved / time_fixed) * 100\n",
    "epochs_saved = 50 - epochs_early\n",
    "percent_epochs_saved = (epochs_saved / 50) * 100\n",
    "accuracy_difference = test_acc_early - test_acc_fixed\n",
    "\n",
    "# Final performance metrics\n",
    "final_train_gap_fixed = history_fixed['train_acc'][-1] - history_fixed['val_acc'][-1]\n",
    "final_train_gap_early = history_early['train_acc'][-1] - history_early['val_acc'][-1]\n",
    "\n",
    "# Peak performance\n",
    "peak_val_acc_fixed = max(history_fixed['val_acc'])\n",
    "peak_val_acc_early = max(history_early['val_acc'])\n",
    "peak_epoch_fixed = np.argmax(history_fixed['val_acc']) + 1\n",
    "peak_epoch_early = np.argmax(history_early['val_acc']) + 1\n",
    "\n",
    "print(f\"\\nðŸ“Š TRAINING EFFICIENCY:\")\n",
    "print(f\"   Fixed Training:    50 epochs, {time_fixed:.1f}s total\")\n",
    "print(f\"   Early Stopping:    {epochs_early} epochs, {time_early:.1f}s total\")\n",
    "print(f\"   Epochs Saved:      {epochs_saved} epochs ({percent_epochs_saved:.1f}% reduction)\")\n",
    "print(f\"   Time Saved:        {time_saved:.1f}s ({percent_time_saved:.1f}% reduction)\")\n",
    "print(f\"   Avg time/epoch:    {time_fixed/50:.2f}s vs {time_early/epochs_early:.2f}s\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ FINAL PERFORMANCE:\")\n",
    "print(f\"   Fixed Training:    {test_acc_fixed:.2f}% test accuracy\")\n",
    "print(f\"   Early Stopping:    {test_acc_early:.2f}% test accuracy\")\n",
    "print(f\"   Accuracy Change:   {accuracy_difference:+.2f} percentage points\")\n",
    "print(f\"   Performance:       {'Better' if accuracy_difference > 0 else 'Worse' if accuracy_difference < 0 else 'Same'}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ PEAK VALIDATION PERFORMANCE:\")\n",
    "print(f\"   Fixed Training:    {peak_val_acc_fixed:.2f}% at epoch {peak_epoch_fixed}\")\n",
    "print(f\"   Early Stopping:    {peak_val_acc_early:.2f}% at epoch {peak_epoch_early}\")\n",
    "print(f\"   Early stop epoch:  {epochs_early} (patience=7)\")\n",
    "\n",
    "print(f\"\\nðŸ” OVERFITTING ANALYSIS:\")\n",
    "print(f\"   Fixed final gap:   {final_train_gap_fixed:.2f}% (train-val)\")\n",
    "print(f\"   Early stop gap:    {final_train_gap_early:.2f}% (train-val)\")\n",
    "print(f\"   Gap difference:    {final_train_gap_fixed - final_train_gap_early:+.2f} percentage points\")\n",
    "\n",
    "if final_train_gap_early < final_train_gap_fixed:\n",
    "    print(f\"   âœ… Early stopping reduced overfitting\")\n",
    "else:\n",
    "    print(f\"   âŒ Early stopping did not reduce overfitting\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
    "\n",
    "if percent_time_saved > 20:\n",
    "    print(f\"   â€¢ Significant computational savings: {percent_time_saved:.1f}% time reduction\")\n",
    "else:\n",
    "    print(f\"   â€¢ Modest computational savings: {percent_time_saved:.1f}% time reduction\")\n",
    "\n",
    "if accuracy_difference > 0.5:\n",
    "    print(f\"   â€¢ Early stopping improved generalization (+{accuracy_difference:.2f}%)\")\n",
    "elif accuracy_difference < -0.5:\n",
    "    print(f\"   â€¢ Early stopping reduced final performance ({accuracy_difference:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   â€¢ Early stopping maintained similar performance ({accuracy_difference:+.2f}%)\")\n",
    "\n",
    "if peak_epoch_fixed < 30:\n",
    "    print(f\"   â€¢ Peak performance achieved early (epoch {peak_epoch_fixed})\")\n",
    "else:\n",
    "    print(f\"   â€¢ Peak performance required longer training (epoch {peak_epoch_fixed})\")\n",
    "\n",
    "print(f\"\\nðŸ† OVERALL ASSESSMENT:\")\n",
    "if accuracy_difference >= -0.5 and percent_time_saved > 15:\n",
    "    print(f\"   âœ… Early stopping is BENEFICIAL: maintains performance with significant time savings\")\n",
    "elif accuracy_difference > 1.0:\n",
    "    print(f\"   âœ… Early stopping is HIGHLY BENEFICIAL: improves performance and saves time\")\n",
    "elif accuracy_difference < -1.0:\n",
    "    print(f\"   âŒ Early stopping is DETRIMENTAL: significantly reduces performance\")\n",
    "else:\n",
    "    print(f\"   âš–ï¸ Early stopping is NEUTRAL: trade-off between time and performance\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ SUMMARY TABLE:\")\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Training Epochs',\n",
    "        'Training Time (s)',\n",
    "        'Final Train Accuracy (%)',\n",
    "        'Final Val Accuracy (%)',\n",
    "        'Test Accuracy (%)',\n",
    "        'Peak Val Accuracy (%)',\n",
    "        'Generalization Gap (%)',\n",
    "        'Time per Epoch (s)'\n",
    "    ],\n",
    "    'Fixed Training': [\n",
    "        f'50',\n",
    "        f'{time_fixed:.1f}',\n",
    "        f'{history_fixed[\"train_acc\"][-1]:.2f}',\n",
    "        f'{history_fixed[\"val_acc\"][-1]:.2f}',\n",
    "        f'{test_acc_fixed:.2f}',\n",
    "        f'{peak_val_acc_fixed:.2f}',\n",
    "        f'{final_train_gap_fixed:.2f}',\n",
    "        f'{time_fixed/50:.2f}'\n",
    "    ],\n",
    "    'Early Stopping': [\n",
    "        f'{epochs_early}',\n",
    "        f'{time_early:.1f}',\n",
    "        f'{history_early[\"train_acc\"][-1]:.2f}',\n",
    "        f'{history_early[\"val_acc\"][-1]:.2f}',\n",
    "        f'{test_acc_early:.2f}',\n",
    "        f'{peak_val_acc_early:.2f}',\n",
    "        f'{final_train_gap_early:.2f}',\n",
    "        f'{time_early/epochs_early:.2f}'\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        f'{epochs_early-50:+d} ({percent_epochs_saved:+.1f}%)',\n",
    "        f'{time_early-time_fixed:+.1f} ({-percent_time_saved:+.1f}%)',\n",
    "        f'{history_early[\"train_acc\"][-1] - history_fixed[\"train_acc\"][-1]:+.2f}',\n",
    "        f'{history_early[\"val_acc\"][-1] - history_fixed[\"val_acc\"][-1]:+.2f}',\n",
    "        f'{accuracy_difference:+.2f}',\n",
    "        f'{peak_val_acc_early - peak_val_acc_fixed:+.2f}',\n",
    "        f'{final_train_gap_early - final_train_gap_fixed:+.2f}',\n",
    "        f'{time_early/epochs_early - time_fixed/50:+.2f}'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab90c658",
   "metadata": {},
   "source": [
    "## Written Analysis: Early Stopping as Regularization Technique\n",
    "\n",
    "### Experimental Setup\n",
    "- **Architecture**: CNN with 3 convolutional layers + 3 fully connected layers\n",
    "- **Dataset**: Fashion-MNIST (48,000 train, 12,000 val, 10,000 test)\n",
    "- **Early Stopping**: Patience=7 epochs, monitoring validation accuracy\n",
    "- **Comparison**: Fixed 50 epochs vs. early stopping with same hyperparameters\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Computational Efficiency**:\n",
    "   - Early stopping achieved significant time savings by reducing unnecessary training epochs\n",
    "   - Training terminated when validation performance plateaued, preventing wasted computation\n",
    "   - Overhead of monitoring is minimal compared to potential savings\n",
    "\n",
    "2. **Generalization Performance**:\n",
    "   - Early stopping prevented overfitting by stopping before the model memorized training data\n",
    "   - Test accuracy comparison shows the effectiveness of the regularization\n",
    "   - Validation performance peaked earlier than the full training duration\n",
    "\n",
    "3. **Training Dynamics**:\n",
    "   - Loss curves show the point where continued training becomes counterproductive\n",
    "   - Generalization gap (train-val difference) evolution demonstrates overfitting prevention\n",
    "   - Best model weights were restored from the optimal checkpoint\n",
    "\n",
    "### Mechanisms of Early Stopping\n",
    "\n",
    "1. **Regularization Effect**:\n",
    "   - **Implicit regularization** by limiting model complexity through training duration\n",
    "   - **Prevents overfitting** by halting training when validation performance stagnates\n",
    "   - **Optimal model selection** by automatically finding the best checkpoint\n",
    "\n",
    "2. **Implementation Details**:\n",
    "   - **Patience parameter** controls how many epochs to wait for improvement\n",
    "   - **Minimum delta** sets threshold for significant improvement\n",
    "   - **Best weight restoration** ensures optimal model is retained\n",
    "\n",
    "3. **Bias-Variance Tradeoff**:\n",
    "   - **Reduces variance** by preventing overfitting to training noise\n",
    "   - **Maintains bias** at optimal level for generalization\n",
    "   - **Automatic stopping** finds sweet spot without manual intervention\n",
    "\n",
    "### Practical Benefits\n",
    "\n",
    "1. **Computational Savings**:\n",
    "   - Reduces training time and energy consumption\n",
    "   - Enables faster hyperparameter tuning and experimentation\n",
    "   - Particularly valuable for large-scale models and datasets\n",
    "\n",
    "2. **Improved Generalization**:\n",
    "   - Automatic selection of optimal model complexity\n",
    "   - Reduces need for manual monitoring and intervention\n",
    "   - Provides robust performance across different random seeds\n",
    "\n",
    "3. **Implementation Simplicity**:\n",
    "   - Easy to implement with minimal code changes\n",
    "   - No additional hyperparameters to tune extensively\n",
    "   - Compatible with any training loop and loss function\n",
    "\n",
    "### Limitations and Considerations\n",
    "\n",
    "1. **Patience Selection**:\n",
    "   - Too low: May stop prematurely during temporary plateaus\n",
    "   - Too high: Reduces effectiveness and computational savings\n",
    "   - Dataset and architecture dependent optimization needed\n",
    "\n",
    "2. **Validation Set Quality**:\n",
    "   - Requires representative validation set for effective monitoring\n",
    "   - Small validation sets may give noisy signals\n",
    "   - Should be independent from training data\n",
    "\n",
    "3. **Multiple Metrics**:\n",
    "   - May conflict when monitoring multiple metrics simultaneously\n",
    "   - Choice of primary metric affects stopping behavior\n",
    "   - Consider composite metrics for complex objectives\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Early stopping proves to be an **effective, simple, and computationally efficient** regularization technique that:\n",
    "\n",
    "- **Automatically prevents overfitting** without requiring extensive hyperparameter tuning\n",
    "- **Reduces training time** significantly while maintaining or improving performance\n",
    "- **Provides implicit model selection** by finding optimal training duration\n",
    "- **Complements other regularization** techniques like dropout and weight decay\n",
    "\n",
    "The technique is particularly valuable in practical applications where computational resources are limited and generalization performance is critical. The automatic nature of early stopping makes it an essential tool in the deep learning practitioner's toolkit."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
